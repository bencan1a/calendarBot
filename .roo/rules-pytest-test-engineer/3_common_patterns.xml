<?xml version="1.0" encoding="utf-8"?>
<common_patterns>
  <overview>
    Reusable pytest patterns and examples for the Pytest Test Engineer mode.
    Include these patterns when creating or refactoring tests in the project.
  </overview>

  <fixture_patterns>
    <pattern name="simple_fixture">
      <description>Minimal fixture pattern for reusable setup/teardown.</description>
      <code language="python"><![CDATA[
# conftest.py
import pytest

@pytest.fixture
def simple_settings(tmp_path):
    """Lightweight settings fixture used by many unit tests."""
    return {"data_dir": tmp_path / "data", "timeout": 1}
]]></code>
      <rationale>Keep fixtures focused and fast; prefer explicit usage over autouse where possible.</rationale>
    </pattern>

    <pattern name="factory_fixture">
      <description>Factory-style fixtures to build test objects without duplication.</description>
      <code language="python"><![CDATA[
# tests/factories.py
import factory

class EventFactory(factory.Factory):
    class Meta:
        model = dict

    id = factory.Sequence(lambda n: n)
    title = "Test event"
    start = "2025-01-01T09:00:00"

# usage in tests:
# def test_event_processing(event_factory):
#     ev = event_factory()
#     assert process(ev) == ...
]]></code>
      <rationale>Factories reduce duplication and make tests easier to read and parametrize.</rationale>
    </pattern>
  </fixture_patterns>

  <parametrization_examples>
    <example name="small_matrix">
      <description>Parametrize small matrices to cover important combinations without explosion.</description>
      <code language="python"><![CDATA[
import pytest

@pytest.mark.parametrize("input,expected", [
    ("a", 1),
    ("b", 2),
])
def test_map_input_to_value(input, expected):
    assert map_input(input) == expected
]]></code>
      <rationale>Prefer readable parametrization; avoid Cartesian products that create too many cases.</rationale>
    </example>
  </parametrization_examples>

  <mocking_patterns>
    <rule>Mock external network/hardware only; prefer exercising in-process components for integration fidelity.</rule>
    <example name="scoped_mock">
      <description>Use monkeypatch or requests-mock in a test-scoped way and assert on external call contract.</description>
      <code language="python"><![CDATA[
def test_external_api(monkeypatch):
    called = {}
    def fake_get(url, timeout):
        called['url'] = url
        return SimpleNamespace(status_code=200, json=lambda: {"ok": True})
    monkeypatch.setattr("calendarbot.utils.http_client.requests.get", fake_get)
    result = fetch_remote()
    assert result == {"ok": True}
    assert called['url'].endswith("/health")
]]></code>
    </example>
  </mocking_patterns>

  <smoke_test_example>
    <description>Lightweight boot smoke test (lite app) â€” always include and mark as smoke.</description>
    <code language="python"><![CDATA[
import subprocess
import time
import pytest
from pathlib import Path

@pytest.mark.smoke
def test_lite_app_boot_no_errors(tmp_path):
    # Run the lite server in a subprocess with a short timeout and capture stderr
    proc = subprocess.Popen(
        ["python", "-m", "calendarbot_lite", "--port", "0"],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        cwd=Path(__file__).parent.parent.parent
    )
    try:
        time.sleep(2)  # short wait for initialization
        stderr = proc.stderr.read().decode("utf-8", errors="ignore")
        assert "ERROR" not in stderr
    finally:
        proc.terminate()
        proc.wait(timeout=5)
]]></code>
    <guidance>Keep this test strict but short; if startup requires env vars, document them in test docstring.</guidance>
  </smoke_test_example>

  <integration_test_pattern>
    <description>Integration test pattern that exercises cross-component flow with clear setup/teardown.</description>
    <code language="python"><![CDATA[
@pytest.mark.integration
def test_whats_next_backend_frontend_consistency(test_settings, tmp_path):
    # Prepare a minimal backend dataset and call the view logic
    settings = test_settings
    backend_result = backend_generate_calendar(settings)
    view_json = render_whats_next(backend_result)
    assert "events" in view_json
    # Additional assertions on real outputs rather than mocked internals
]]></code>
    <rationale>Integration tests should validate behavior across modules and avoid mocking internal calls.</rationale>
  </integration_test_pattern>

  <assertion_guidance>
    <rule>Assert public outputs and side effects rather than internal calls.</rule>
    <critical_rule>ALL assertions must be unconditional - no if/else in test body.</critical_rule>
    <examples>
      <item>Prefer assert render_output(...) == expected_json</item>
      <item>Avoid asserting that a specific internal function was called unless contract requires it</item>
    </examples>
  </assertion_guidance>

  <anti_pattern_examples>
    <overview>Common anti-patterns found in CalendarBot test suite with corrections. See docs/pytest-best-practices.md for complete guide.</overview>

    <anti_pattern name="conditional_assertions">
      <bad><![CDATA[
# BAD - assertions may not execute
def test_process_events():
    result = process(events)
    if len(result.events) > 0:
        assert result.events[0].status == "active"  # Skipped if empty!
    else:
        pass  # Test passes without checking anything
]]></bad>
      <good><![CDATA[
# GOOD - unconditional, tests specific outcome
def test_process_events():
    result = process(events)
    assert len(result.events) == 2  # Always executes
    assert result.events[0].status == "active"
    assert result.events[1].status == "pending"
]]></good>
    </anti_pattern>

    <anti_pattern name="accepting_multiple_outcomes">
      <bad><![CDATA[
# BAD - accepts any boolean value
def test_parser():
    result = parse(ics_content)
    assert result.success is True or result.success is False  # Always passes!
]]></bad>
      <good><![CDATA[
# GOOD - tests specific expected outcome
def test_parser():
    result = parse(valid_ics_content)
    assert result.success is True
    assert result.event_count == 5
    assert result.error_message is None
]]></good>
    </anti_pattern>

    <anti_pattern name="over_mocking">
      <bad><![CDATA[
# BAD - mocks business logic
def test_event_filter(mocker):
    mock_filter = mocker.patch("app.filter_events")
    mock_filter.return_value = [event1]  # Not testing actual filter!
    result = process_calendar(events)
    assert len(result) == 1
]]></bad>
      <good><![CDATA[
# GOOD - tests real filtering logic
def test_event_filter():
    busy_event = Event(status="busy")
    free_event = Event(status="free")
    result = filter_busy_events([busy_event, free_event])
    assert result == [busy_event]  # Verifies actual logic
]]></good>
    </anti_pattern>

    <anti_pattern name="weak_assertions">
      <bad><![CDATA[
# BAD - only checks type, not value
def test_correlation_id():
    result = handler.process(request)
    assert isinstance(result.correlation_id, str)  # Passes with any string!
]]></bad>
      <good><![CDATA[
# GOOD - verifies actual value propagation
def test_correlation_id():
    result = handler.process(request_id="test-123")
    assert result.correlation_id == "test-123"  # Would fail if propagation breaks
]]></good>
    </anti_pattern>

    <anti_pattern name="testing_effects_not_causes">
      <bad><![CDATA[
# BAD - only checks effect
def test_event_filtering():
    result = parse_and_filter(ics_content)
    assert len(result.events) == 0  # WHY is it empty?
]]></bad>
      <good><![CDATA[
# GOOD - verifies cause of empty result
def test_event_filtering():
    result = parse_and_filter(ics_content_with_only_free_events)
    assert result.success is True
    assert result.events == []
    assert result.filtered_count == 3  # Shows WHY empty
    assert "filtered 3 FREE events" in result.message
]]></good>
    </anti_pattern>
  </anti_pattern_examples>

  <three_question_validation>
    <overview>Before committing any test, ask these three questions:</overview>
    <question number="1">
      <text>Does this test verify BEHAVIOR (not just types)?</text>
      <bad>assert isinstance(event.id, str)</bad>
      <good>assert event.id == "expected-123"</good>
    </question>
    <question number="2">
      <text>Will this test FAIL if the production code breaks?</text>
      <bad>assert len(events) >= 0  # Always true</bad>
      <good>assert len(events) == 5  # Fails if logic changes</good>
    </question>
    <question number="3">
      <text>Are ALL assertions UNCONDITIONAL (no if statements)?</text>
      <bad>if result: assert result.status == "ok"</bad>
      <good>assert result is not None; assert result.status == "ok"</good>
    </question>
  </three_question_validation>

  <update_tests_over_impl>
    <rule>When behavior differs from tests, update tests to match intended functionality. Never change production code solely to satisfy a test that encodes incorrect behavior.</rule>
    <process>
      <step>Confirm intended behavior with documentation or maintainers.</step>
      <step>Update or add tests that express the intended behavior.</step>
      <step>If production code has a bug, produce a code change with tests that demonstrate the fixed behavior (separate PR).</step>
    </process>
  </update_tests_over_impl>
</common_patterns>